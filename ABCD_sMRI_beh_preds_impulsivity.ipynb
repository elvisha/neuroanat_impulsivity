{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain-Based Predictive Modeling | Neuroanatomy x Impulsivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook outlines the brain-based predictive modeling framework when using sMRI data to predict impulsivity-related behaviors in the ABCD data. \n",
    "\n",
    "Specific sections need to be customized for different analyses. Details can be found within each of the individual sections and cells.\n",
    "\n",
    "Elvisha Dhamala, 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys; sys.path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit, StratifiedKFold\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Subgroup and Time Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify sex\n",
    "pred_group = 'm'\n",
    "group_var=1\n",
    "\n",
    "#pred_group = all if including all participants\n",
    "#demo_sex_v2 for sex, 1=M, 2=F, 3=Intersex-M, 4=Intersex-F; 999=Don't know; 777=Refuse to answer\n",
    "#pred_group = m or f for sex\n",
    "#race_ethnicity for race/ethnciity, 1 = White; 2 = Black; 3 = Hispanic; 4 = Asian; 5 = Other\n",
    "#pred_group = wh, bl, hi, as, ot for race/ethnicity\n",
    "\n",
    "#set the time point you want to consider in your analyses\n",
    "timepoint = 'baseline_year_1_arm_1'\n",
    "#for baseline: 'baseline_year_1_arm_1'\n",
    "#for 2y follow-up: '2_year_follow_up_y_arm_1'\n",
    "#for 4y follow-up: '4_year_follow_up_y_arm_1'\n",
    "#for 6y follow-up: '6_year_follow_up_y_arm_1'\n",
    "pred_yr = '0y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Imaging Data and Generic ABCD Data (Site, Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and clean up ABCD data\n",
    "# set base directories\n",
    "base_dir   = '/Users/elvishadhamala/Documents/BPM/abcd-data-release-5.1/'\n",
    "BPM_base_dir = '/Users/elvishadhamala/Documents/BPM/'\n",
    "\n",
    "#load general subj site, qc, and demo data\n",
    "lt = pd.read_csv(os.path.join(base_dir, 'core/abcd-general/abcd_y_lt.csv'), header=0)\n",
    "qc = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_qc_incl.csv'), header=0)\n",
    "clfind = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_qc_clfind.csv'), header=0)\n",
    "demo = pd.read_csv(os.path.join(base_dir, 'core/abcd-general/abcd_p_demo.csv'), header=0)\n",
    "icv = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_smr_vol_aseg.csv'), header=0)\n",
    "\n",
    "#load brain and behavior data\n",
    "ct = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_smr_thk_dsk.csv'), header=0) \n",
    "sa = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_smr_area_dsk.csv'), header=0) \n",
    "vol = pd.read_csv(os.path.join(base_dir, 'core/imaging/mri_y_smr_vol_dsk.csv'), header=0) \n",
    "\n",
    "#only keep relevant data for qc, site, demo\n",
    "qc = qc[['src_subject_id', 'eventname', 'imgincl_t1w_include']]\n",
    "icv = icv[['src_subject_id', 'eventname', 'smri_vol_scs_intracranialv']]\n",
    "site = lt[['src_subject_id', 'eventname', 'site_id_l']]\n",
    "demo = demo[['src_subject_id', 'eventname', 'demo_sex_v2', 'race_ethnicity']]\n",
    "fam = lt[['src_subject_id', 'eventname', 'rel_family_id']]\n",
    "\n",
    "#demo and family data was collected at baseline so just consider that\n",
    "demo = demo[demo.eventname == 'baseline_year_1_arm_1']\n",
    "fam = fam[fam.eventname == 'baseline_year_1_arm_1']\n",
    "\n",
    "\n",
    "#get rid of participants wtih poor quality img data\n",
    "qc = qc[qc.imgincl_t1w_include == 1]\n",
    "clfind = clfind[clfind.mrif_score < 3]\n",
    "\n",
    "#remove mean columns from ct, sa, and vol\n",
    "ct = ct.drop(columns=['smri_thick_cdk_meanlh', 'smri_thick_cdk_meanrh', 'smri_thick_cdk_mean'])\n",
    "sa = sa.drop(columns=['smri_area_cdk_totallh', 'smri_area_cdk_totalrh', 'smri_area_cdk_total'])\n",
    "vol = vol.drop(columns=['smri_vol_cdk_totallh', 'smri_vol_cdk_totalrh', 'smri_vol_cdk_total'])\n",
    "\n",
    "\n",
    "#sort data just to make sure they're all in the same order\n",
    "qc = qc.sort_values(by='src_subject_id', ascending=True)\n",
    "icv = icv.sort_values(by='src_subject_id', ascending=True)\n",
    "ct = ct.sort_values(by='src_subject_id', ascending=True)\n",
    "sa = sa.sort_values(by='src_subject_id', ascending=True)\n",
    "vol = vol.sort_values(by='src_subject_id', ascending=True)\n",
    "site = site.sort_values(by='src_subject_id', ascending=True)\n",
    "fam = fam.sort_values(by='src_subject_id', ascending=True)\n",
    "clfind = clfind.sort_values(by='src_subject_id', ascending=True)\n",
    "demo = demo.sort_values(by='src_subject_id', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Behavioral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the behavioral data to be used for the analyses and clean it up based on the variables needed\n",
    "beh1 = pd.read_csv(os.path.join(base_dir, 'core/mental-health/mh_y_bisbas.csv'), header=0)\n",
    "beh1 = beh1[['src_subject_id', 'eventname', \n",
    "           'bis_y_ss_bis_sum', \n",
    "           'bis_y_ss_bas_rr', 'bis_y_ss_bas_drive', 'bis_y_ss_bas_fs']]\n",
    "beh1 = beh1.dropna(axis='rows')\n",
    "beh1 = beh1.sort_values(by='src_subject_id', ascending=True)\n",
    "\n",
    "beh2 = pd.read_csv(os.path.join(base_dir, 'core/mental-health/mh_y_upps.csv'), header=0)\n",
    "beh2 = beh2[['src_subject_id', 'eventname', \n",
    "           'upps_y_ss_negative_urgency', 'upps_y_ss_lack_of_planning', 'upps_y_ss_sensation_seeking',\n",
    "           'upps_y_ss_positive_urgency', 'upps_y_ss_lack_of_perseverance']]\n",
    "beh2 = beh2.dropna(axis='rows')\n",
    "beh2 = beh2.sort_values(by='src_subject_id', ascending=True)\n",
    "\n",
    "#set name of behavioral prediction domain and specify where to store the results data\n",
    "results_dir   = '/Users/elvishadhamala/Documents/BPM/preds/preds_bisbas/'\n",
    "beh_var = 'bisbas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Time Point(s) to be Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from specific time point\n",
    "qc = qc[qc.eventname == timepoint]\n",
    "icv = icv[icv.eventname == timepoint]\n",
    "cl = clfind[clfind.eventname == timepoint]\n",
    "ct = ct[ct.eventname == timepoint]\n",
    "sa = sa[sa.eventname == timepoint]\n",
    "vol = vol[vol.eventname == timepoint]\n",
    "site = site[site.eventname == timepoint]\n",
    "beh1 = beh1[beh1.eventname == timepoint]\n",
    "beh2 = beh2[beh2.eventname == timepoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate Participants with Complete Brain and Behavioral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out subjs with complete data across all brain and behavioral variables of interest\n",
    "series1 = pd.Series(qc.src_subject_id)\n",
    "series2 = pd.Series(icv.src_subject_id)\n",
    "series3 = pd.Series(cl.src_subject_id)\n",
    "series4 = pd.Series(ct.src_subject_id)\n",
    "series5 = pd.Series(sa.src_subject_id)\n",
    "series6 = pd.Series(vol.src_subject_id)\n",
    "series7 = pd.Series(beh1.src_subject_id)\n",
    "series8 = pd.Series(beh2.src_subject_id)\n",
    "series9 = pd.Series(demo.src_subject_id)\n",
    "\n",
    "\n",
    "subj_complete = (set(series1) & set(series2) & set(series3) & set(series4) & \n",
    "                 set(series5) & set(series6) & set(series7) & set(series8) & set(series9))\n",
    "subj_complete = pd.DataFrame(subj_complete)\n",
    "subj_complete.columns = ['s']\n",
    "\n",
    "fam = fam[fam.src_subject_id.isin(subj_complete.s)]\n",
    "fam_unrelated = fam.drop_duplicates(subset=['rel_family_id'], keep='first')\n",
    "subj_complete_unrelated = pd.DataFrame(fam_unrelated.src_subject_id)\n",
    "subj_complete_unrelated.columns = ['s']\n",
    "\n",
    "\n",
    "#isolate data just from participants of interest (i.e., participants with all data)\n",
    "icv = icv[icv.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "ct = ct[ct.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "sa = sa[sa.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "vol = vol[vol.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "site = site[site.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "demo = demo[demo.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "beh1 = beh1[beh1.src_subject_id.isin(subj_complete_unrelated.s)]\n",
    "beh2 = beh2[beh2.src_subject_id.isin(subj_complete_unrelated.s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Irrelevant Columns from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove subj id and eventname columns, change to array\n",
    "ct = ct.drop(columns=['src_subject_id', 'eventname'])\n",
    "sa = sa.drop(columns=['src_subject_id', 'eventname'])\n",
    "vol = vol.drop(columns=['src_subject_id', 'eventname'])\n",
    "site = site.drop(columns=['src_subject_id', 'eventname'])\n",
    "icv = icv.drop(columns=['src_subject_id', 'eventname'])\n",
    "beh1 = beh1.drop(columns=['src_subject_id', 'eventname'])\n",
    "beh2 = beh2.drop(columns=['src_subject_id', 'eventname'])\n",
    "\n",
    "#use specifivc behavioral variable of interest\n",
    "beh = beh1\n",
    "\n",
    "#store a list of vars that are in the beh data that will be predicted\n",
    "beh_names = beh.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Mask to Data As Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mask based on whatever population subgroup you're looking to model \n",
    "mask = demo.demo_sex_v2 == group_var\n",
    "#mask = ~np.isnan(demo.demo_sex_v2)\n",
    "\n",
    "ct = ct[mask.values].values\n",
    "sa = sa[mask.values].values\n",
    "vol = vol[mask.values].values\n",
    "beh = beh[mask.values].values\n",
    "site = site[mask.values].values\n",
    "icv = icv[mask.values].values\n",
    "\n",
    "#apply proportional correction to icv data\n",
    "sa = sa/icv\n",
    "vol = vol/icv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run The Predictive Models | Cortical Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with cortical thickness data\n",
    "X = ct\n",
    "brain_var = 'ct'\n",
    "\n",
    "#set y data to be beh\n",
    "Y = beh\n",
    "\n",
    "#set site data to be site info\n",
    "site = site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .80\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#number of features \n",
    "n_feat = X.shape[1]\n",
    "\n",
    "#number of behaviors to predict\n",
    "n_beh = Y.shape[1]\n",
    "\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha = np.zeros([n_beh, rep])\n",
    "#feature importance extracted from the model\n",
    "featimp = np.zeros([n_beh, rep, n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe = np.zeros([n_beh, rep, n_feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    beh_data = Y[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of repetitions\n",
    "    for p in range(rep):\n",
    "    \n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "    \n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        #set site values based on indices from split\n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores = []\n",
    "        best_params = []\n",
    "\n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "\n",
    "\n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, \n",
    "                                  verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "    \n",
    "        #fit regressor to the model, use site ID as group category again\n",
    "        gridSearch.fit(x_train, y_train, groups=site_train)\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params.append(list(gridSearch.best_params_.values()))\n",
    "        best_scores.append(gridSearch.best_score_)\n",
    "\n",
    "        #save optimised alpha values\n",
    "        opt_alpha[b,p] = best_params[best_scores.index(np.max(best_scores))][0]\n",
    "\n",
    "        #rand_alpha = np.random.choice(alphas)\n",
    "\n",
    "        #fit optimized model\n",
    "        model = Ridge(alpha = opt_alpha[b,p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "        #haufe-transform feature weights\n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        y_train_preds = []\n",
    "        y_train_preds = model.predict(x_train).ravel()\n",
    "\n",
    "        #extract feature importance\n",
    "        featimp[b,p,:] = model.coef_\n",
    "        #compute Haufe-inverted feature weights\\\n",
    "        #cov x is the covariance of the x_train data - covariance of brain data for the training set\n",
    "        cov_x = np.cov(x_train.T)\n",
    "        #cov y is the covariance of the y_train predicted data - covariance of the predicted behavior for the training set\n",
    "        cov_y = np.cov(y_train_preds)\n",
    "        #this is from eqn 6 of the haufe 2014 paper\n",
    "        featimp_haufe[b,p,:] = np.matmul(cov_x,featimp[b,p,:])*(1/cov_y)\n",
    "\n",
    "\n",
    "    #save results\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2.npy'),r2)\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var.npy'),var)\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr.npy'),corr)\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_optalpha.npy'),opt_alpha)\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimp.npy'),featimp)\n",
    "    #np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimphaufe.npy'),featimp_haufe)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run the Null Models | Cortical Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most params stay the same as the tru models, but # of reps increases\n",
    "rep = 1000\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    #shuffle the data within the sites\n",
    "    #need to create a new array to store those values so they don't just overwrite the previous one\n",
    "    values = list(Y)\n",
    "    values = np.array(values)\n",
    "    groups = site\n",
    "\n",
    "    for index in np.unique(groups):\n",
    "        mask = groups==index\n",
    "        mask = mask.reshape(-1)\n",
    "        values[mask] = np.random.permutation(values[mask,:])\n",
    "    Y_shuffle = values\n",
    "    beh_data = Y_shuffle[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of models\n",
    "    for p in range(rep):\n",
    "\n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "\n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "    \n",
    "        #randomly choose an alpha value based on the optimized alphas\n",
    "        rand_alpha = np.random.choice(opt_alpha[b,:])\n",
    "\n",
    "        #fit optimized models\n",
    "        model = Ridge(alpha = rand_alpha, normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "    #save results   \n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2_null.npy'),r2)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var_null.npy'),var)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr_null.npy'),corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run The Predictive Models | Surface Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with cortical thickness data\n",
    "X = sa\n",
    "brain_var = 'sa'\n",
    "\n",
    "#set y data to be beh\n",
    "Y = beh\n",
    "\n",
    "#set site data to be site info\n",
    "site = site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .80\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#number of features \n",
    "n_feat = X.shape[1]\n",
    "\n",
    "#number of behaviors to predict\n",
    "n_beh = Y.shape[1]\n",
    "\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha = np.zeros([n_beh, rep])\n",
    "#feature importance extracted from the model\n",
    "featimp = np.zeros([n_beh, rep, n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe = np.zeros([n_beh, rep, n_feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    beh_data = Y[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of repetitions\n",
    "    for p in range(rep):\n",
    "    \n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "    \n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        #set site values based on indices from split\n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores = []\n",
    "        best_params = []\n",
    "\n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "\n",
    "\n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "    \n",
    "        #fit regressor to the model, use site ID as group category again\n",
    "        gridSearch.fit(x_train, y_train, groups=site_train)\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params.append(list(gridSearch.best_params_.values()))\n",
    "        best_scores.append(gridSearch.best_score_)\n",
    "\n",
    "        #save optimised alpha values\n",
    "        opt_alpha[b,p] = best_params[best_scores.index(np.max(best_scores))][0]\n",
    "\n",
    "        #rand_alpha = np.random.choice(alphas)\n",
    "\n",
    "        #fit optimized model\n",
    "        model = Ridge(alpha = opt_alpha[b,p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "        #haufe-transform feature weights\n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        y_train_preds = []\n",
    "        y_train_preds = model.predict(x_train).ravel()\n",
    "\n",
    "        #extract feature importance\n",
    "        featimp[b,p,:] = model.coef_\n",
    "        #compute Haufe-inverted feature weights\\\n",
    "        #cov x is the covariance of the x_train data - covariance of brain data for the training set\n",
    "        cov_x = np.cov(x_train.T)\n",
    "        #cov y is the covariance of the y_train predicted data - covariance of the predicted behavior for the training set\n",
    "        cov_y = np.cov(y_train_preds)\n",
    "        #this is from eqn 6 of the haufe 2014 paper\n",
    "        featimp_haufe[b,p,:] = np.matmul(cov_x,featimp[b,p,:])*(1/cov_y)\n",
    "\n",
    "\n",
    "    #save results\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2.npy'),r2)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var.npy'),var)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr.npy'),corr)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_optalpha.npy'),opt_alpha)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimp.npy'),featimp)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimphaufe.npy'),featimp_haufe)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run the Null Models | Surface Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most params stay the same as the tru models, but # of reps increases\n",
    "rep = 1000\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    #shuffle the data within the sites\n",
    "    #need to create a new array to store those values so they don't just overwrite the previous one\n",
    "    values = list(Y)\n",
    "    values = np.array(values)\n",
    "    groups = site\n",
    "\n",
    "    for index in np.unique(groups):\n",
    "        mask = groups==index\n",
    "        mask = mask.reshape(-1)\n",
    "        values[mask] = np.random.permutation(values[mask,:])\n",
    "    Y_shuffle = values\n",
    "    beh_data = Y_shuffle[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of models\n",
    "    for p in range(rep):\n",
    "\n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "\n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "    \n",
    "        #randomly choose an alpha value based on the optimized alphas\n",
    "        rand_alpha = np.random.choice(opt_alpha[b,:])\n",
    "\n",
    "        #fit optimized models\n",
    "        model = Ridge(alpha = rand_alpha, normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "    #save results   \n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2_null.npy'),r2)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var_null.npy'),var)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr_null.npy'),corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run The Predictive Models | Gray Matter Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with cortical thickness data\n",
    "X = vol\n",
    "brain_var = 'vol'\n",
    "\n",
    "#set y data to be beh\n",
    "Y = beh\n",
    "\n",
    "#set site data to be site info\n",
    "site = site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of repetitions you want to perform\n",
    "rep = 100\n",
    "\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .80\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "regr = Ridge(normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'alpha': alphas}\n",
    "\n",
    "#number of features \n",
    "n_feat = X.shape[1]\n",
    "\n",
    "#number of behaviors to predict\n",
    "n_beh = Y.shape[1]\n",
    "\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha = np.zeros([n_beh, rep])\n",
    "#feature importance extracted from the model\n",
    "featimp = np.zeros([n_beh, rep, n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "featimp_haufe = np.zeros([n_beh, rep, n_feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    beh_data = Y[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of repetitions\n",
    "    for p in range(rep):\n",
    "    \n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "    \n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        #set site values based on indices from split\n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "\n",
    "        #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "        best_scores = []\n",
    "        best_params = []\n",
    "\n",
    "        #set parameters for inner and outer loops for CV\n",
    "        cv_split = GroupKFold(n_splits=k)\n",
    "\n",
    "\n",
    "        #define regressor with grid-search CV for inner loop\n",
    "        gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "    \n",
    "        #fit regressor to the model, use site ID as group category again\n",
    "        gridSearch.fit(x_train, y_train, groups=site_train)\n",
    "\n",
    "        #save parameters corresponding to the best score\n",
    "        best_params.append(list(gridSearch.best_params_.values()))\n",
    "        best_scores.append(gridSearch.best_score_)\n",
    "\n",
    "        #save optimised alpha values\n",
    "        opt_alpha[b,p] = best_params[best_scores.index(np.max(best_scores))][0]\n",
    "\n",
    "        #rand_alpha = np.random.choice(alphas)\n",
    "\n",
    "        #fit optimized model\n",
    "        model = Ridge(alpha = opt_alpha[b,p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "        #haufe-transform feature weights\n",
    "        cov_x = []\n",
    "        cov_y = []\n",
    "        y_train_preds = []\n",
    "        y_train_preds = model.predict(x_train).ravel()\n",
    "\n",
    "        #extract feature importance\n",
    "        featimp[b,p,:] = model.coef_\n",
    "        #compute Haufe-inverted feature weights\\\n",
    "        #cov x is the covariance of the x_train data - covariance of brain data for the training set\n",
    "        cov_x = np.cov(x_train.T)\n",
    "        #cov y is the covariance of the y_train predicted data - covariance of the predicted behavior for the training set\n",
    "        cov_y = np.cov(y_train_preds)\n",
    "        #this is from eqn 6 of the haufe 2014 paper\n",
    "        featimp_haufe[b,p,:] = np.matmul(cov_x,featimp[b,p,:])*(1/cov_y)\n",
    "\n",
    "\n",
    "    #save results\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2.npy'),r2)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var.npy'),var)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr.npy'),corr)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_optalpha.npy'),opt_alpha)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimp.npy'),featimp)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_featimphaufe.npy'),featimp_haufe)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up and Run the Null Models | Gray Matter Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most params stay the same as the tru models, but # of reps increases\n",
    "rep = 1000\n",
    "\n",
    "#create variables to store the results\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([n_beh, rep])\n",
    "#explained variance\n",
    "var = np.zeros([n_beh, rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([n_beh, rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_beh):\n",
    "    #shuffle the data within the sites\n",
    "    #need to create a new array to store those values so they don't just overwrite the previous one\n",
    "    values = list(Y)\n",
    "    values = np.array(values)\n",
    "    groups = site\n",
    "\n",
    "    for index in np.unique(groups):\n",
    "        mask = groups==index\n",
    "        mask = mask.reshape(-1)\n",
    "        values[mask] = np.random.permutation(values[mask,:])\n",
    "    Y_shuffle = values\n",
    "    beh_data = Y_shuffle[:,b]\n",
    "    print(beh_names[b])\n",
    "    \n",
    "    #iterate through number of models\n",
    "    for p in range(rep):\n",
    "\n",
    "        #split into train/test data\n",
    "        train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state=p).split(X, groups=site))\n",
    "\n",
    "        #set x values based on indices from split\n",
    "        x_train = X[train_inds]\n",
    "        x_test = X[test_inds]\n",
    "        \n",
    "        #set y values based on indices from split  \n",
    "        beh_train = beh_data[:][train_inds]\n",
    "        beh_test = beh_data[:][test_inds]\n",
    "    \n",
    "        site_train = site[train_inds]\n",
    "        site_test = site[test_inds] \n",
    "    \n",
    "        #convert y values to to double\n",
    "        y_train = np.double(beh_train)\n",
    "        y_test = np.double(beh_test)\n",
    "    \n",
    "        #randomly choose an alpha value based on the optimized alphas\n",
    "        rand_alpha = np.random.choice(opt_alpha[b,:])\n",
    "\n",
    "        #fit optimized models\n",
    "        model = Ridge(alpha = rand_alpha, normalize=True, max_iter=1000000, solver='lsqr')\n",
    "        model.fit(x_train, y_train);\n",
    "\n",
    "        #evaluate model within sex within behavior\n",
    "        r2[b,p]=model.score(x_test,y_test)\n",
    "\n",
    "        preds = []\n",
    "        preds = model.predict(x_test).ravel()\n",
    "\n",
    "        #compute explained variance \n",
    "        var[b,p] = explained_variance_score(y_test, preds)\n",
    "\n",
    "        #compute correlation between true and predicted (prediction accuracy)\n",
    "        corr[b,p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "\n",
    "    #save results   \n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_r2_null.npy'),r2)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_var_null.npy'),var)\n",
    "    np.save((results_dir + brain_var + '_' + beh_var + '_' + pred_group + '_' + pred_yr + '_corr_null.npy'),corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
